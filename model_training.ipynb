{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cnn_model import CNNModel\n",
    "from vit_model import ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll see if needed\n",
    "# Define transforms to preprocess the data (you can customize these as needed)\n",
    "TARGET_WIDTH = 224\n",
    "TARGET_HEIGHT = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image\n",
    "    #transforms.Pad((111, 96, 112, 96), fill=0),  # Add Padding:  Our images are 64x33 (left,right,top, bottom)\n",
    "    transforms.Pad((96,80, 95, 80), fill=0) # Add padding for ViT (target is 224*224)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'holo': 0, 'non-holo': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset using ImageFolder\n",
    "dataset = datasets.ImageFolder(root=\"dataset\", transform=transform)\n",
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : holo\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 943\n",
    "plt.imshow(dataset[image_index][0].permute(1, 2, 0))\n",
    "print(f\"label : {dataset.classes[dataset[image_index][1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random_split to split the dataset into train, validation, and test sets\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader for each split using the respective sampler\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True )\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 3000, 3000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_tiny_patch16_224_in21k to current vit_tiny_patch16_224.augreg_in21k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "#model = CNNModel(num_classes=1,\n",
    "#                 lr=0.0001)\n",
    "model = ViTModel(num_classes=1,\n",
    "                 lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (pretrained_ViT): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=192, out_features=1, bias=True)\n",
       "  )\n",
       "  (accuracy_metric): BinaryAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"models\",\n",
    "        monitor=\"validation_accuracy\",\n",
    "        filename=\"best\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = L.Trainer(max_epochs=30,\n",
    "                    log_every_n_steps=1,\n",
    "                    val_check_interval=0.25,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/yanis/paris_cite/S2/TER/model_training/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | pretrained_ViT  | VisionTransformer | 5.5 M \n",
      "1 | accuracy_metric | BinaryAccuracy    | 0     \n",
      "------------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.5 M     Total params\n",
      "22.098    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  25%|██▌       | 47/188 [00:23<01:11,  1.96it/s, v_num=2, train_accuracy_step=0.461, validation_loss=0.703, validation_accuracy=0.505]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 47: 'validation_accuracy' reached 0.50467 (best 0.50467), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 94/188 [00:47<00:47,  1.97it/s, v_num=2, train_accuracy_step=0.547, validation_loss=0.642, validation_accuracy=0.616]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 94: 'validation_accuracy' reached 0.61600 (best 0.61600), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████▌  | 141/188 [01:11<00:23,  1.96it/s, v_num=2, train_accuracy_step=0.680, validation_loss=0.612, validation_accuracy=0.648]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 141: 'validation_accuracy' reached 0.64800 (best 0.64800), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 188/188 [01:37<00:00,  1.93it/s, v_num=2, train_accuracy_step=0.750, validation_loss=0.536, validation_accuracy=0.739]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 188: 'validation_accuracy' reached 0.73933 (best 0.73933), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▌       | 47/188 [00:22<01:06,  2.12it/s, v_num=2, train_accuracy_step=0.680, validation_loss=0.537, validation_accuracy=0.723, train_accuracy_epoch=0.608] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 235: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 94/188 [00:45<00:45,  2.08it/s, v_num=2, train_accuracy_step=0.773, validation_loss=0.567, validation_accuracy=0.712, train_accuracy_epoch=0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 282: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 141/188 [01:06<00:22,  2.12it/s, v_num=2, train_accuracy_step=0.742, validation_loss=0.500, validation_accuracy=0.763, train_accuracy_epoch=0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 329: 'validation_accuracy' reached 0.76333 (best 0.76333), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 188/188 [01:27<00:00,  2.16it/s, v_num=2, train_accuracy_step=0.656, validation_loss=0.499, validation_accuracy=0.760, train_accuracy_epoch=0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 376: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  25%|██▌       | 47/188 [00:22<01:06,  2.13it/s, v_num=2, train_accuracy_step=0.773, validation_loss=0.541, validation_accuracy=0.740, train_accuracy_epoch=0.749] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 423: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 94/188 [00:43<00:43,  2.14it/s, v_num=2, train_accuracy_step=0.797, validation_loss=0.492, validation_accuracy=0.771, train_accuracy_epoch=0.749]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 470: 'validation_accuracy' reached 0.77100 (best 0.77100), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 141/188 [01:07<00:22,  2.08it/s, v_num=2, train_accuracy_step=0.781, validation_loss=0.525, validation_accuracy=0.745, train_accuracy_epoch=0.749]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 517: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 188/188 [01:31<00:00,  2.05it/s, v_num=2, train_accuracy_step=0.812, validation_loss=0.597, validation_accuracy=0.719, train_accuracy_epoch=0.749]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 564: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  25%|██▌       | 47/188 [00:20<01:02,  2.24it/s, v_num=2, train_accuracy_step=0.836, validation_loss=0.508, validation_accuracy=0.762, train_accuracy_epoch=0.790] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 611: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|█████     | 94/188 [00:42<00:42,  2.21it/s, v_num=2, train_accuracy_step=0.812, validation_loss=0.517, validation_accuracy=0.760, train_accuracy_epoch=0.790]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 658: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  75%|███████▌  | 141/188 [01:03<00:21,  2.23it/s, v_num=2, train_accuracy_step=0.773, validation_loss=0.503, validation_accuracy=0.775, train_accuracy_epoch=0.790]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 705: 'validation_accuracy' reached 0.77500 (best 0.77500), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 188/188 [01:24<00:00,  2.23it/s, v_num=2, train_accuracy_step=0.797, validation_loss=0.495, validation_accuracy=0.766, train_accuracy_epoch=0.790]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 752: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  25%|██▌       | 47/188 [00:21<01:03,  2.23it/s, v_num=2, train_accuracy_step=0.797, validation_loss=0.512, validation_accuracy=0.763, train_accuracy_epoch=0.818] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 799: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  50%|█████     | 94/188 [00:41<00:41,  2.25it/s, v_num=2, train_accuracy_step=0.891, validation_loss=0.501, validation_accuracy=0.775, train_accuracy_epoch=0.818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 846: 'validation_accuracy' reached 0.77533 (best 0.77533), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  75%|███████▌  | 141/188 [01:02<00:20,  2.24it/s, v_num=2, train_accuracy_step=0.859, validation_loss=0.528, validation_accuracy=0.759, train_accuracy_epoch=0.818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 893: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 188/188 [01:23<00:00,  2.24it/s, v_num=2, train_accuracy_step=0.828, validation_loss=0.480, validation_accuracy=0.782, train_accuracy_epoch=0.818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 940: 'validation_accuracy' reached 0.78167 (best 0.78167), saving model to '/home/yanis/paris_cite/S2/TER/model_training/models/best-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  25%|██▌       | 47/188 [00:20<01:02,  2.25it/s, v_num=2, train_accuracy_step=0.875, validation_loss=0.560, validation_accuracy=0.770, train_accuracy_epoch=0.840] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 987: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  50%|█████     | 94/188 [00:41<00:41,  2.25it/s, v_num=2, train_accuracy_step=0.867, validation_loss=0.542, validation_accuracy=0.775, train_accuracy_epoch=0.840]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1034: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  75%|███████▌  | 141/188 [01:03<00:21,  2.21it/s, v_num=2, train_accuracy_step=0.859, validation_loss=0.514, validation_accuracy=0.781, train_accuracy_epoch=0.840]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1081: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 188/188 [01:24<00:00,  2.24it/s, v_num=2, train_accuracy_step=0.828, validation_loss=0.505, validation_accuracy=0.770, train_accuracy_epoch=0.840]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1128: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  25%|██▌       | 47/188 [00:20<01:01,  2.28it/s, v_num=2, train_accuracy_step=0.852, validation_loss=0.514, validation_accuracy=0.774, train_accuracy_epoch=0.864] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1175: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  50%|█████     | 94/188 [00:41<00:41,  2.27it/s, v_num=2, train_accuracy_step=0.930, validation_loss=0.566, validation_accuracy=0.770, train_accuracy_epoch=0.864]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1222: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  75%|███████▌  | 141/188 [01:02<00:20,  2.26it/s, v_num=2, train_accuracy_step=0.891, validation_loss=0.537, validation_accuracy=0.777, train_accuracy_epoch=0.864]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1269: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 188/188 [01:23<00:00,  2.26it/s, v_num=2, train_accuracy_step=0.969, validation_loss=0.582, validation_accuracy=0.763, train_accuracy_epoch=0.864]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1316: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  25%|██▌       | 47/188 [00:20<01:02,  2.26it/s, v_num=2, train_accuracy_step=0.953, validation_loss=0.603, validation_accuracy=0.760, train_accuracy_epoch=0.885] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1363: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  50%|█████     | 94/188 [00:42<00:42,  2.24it/s, v_num=2, train_accuracy_step=0.938, validation_loss=0.683, validation_accuracy=0.765, train_accuracy_epoch=0.885]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1410: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  75%|███████▌  | 141/188 [01:03<00:21,  2.24it/s, v_num=2, train_accuracy_step=0.906, validation_loss=0.599, validation_accuracy=0.779, train_accuracy_epoch=0.885]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1457: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 188/188 [01:24<00:00,  2.22it/s, v_num=2, train_accuracy_step=0.906, validation_loss=0.557, validation_accuracy=0.771, train_accuracy_epoch=0.885]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1504: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  25%|██▌       | 47/188 [00:21<01:04,  2.19it/s, v_num=2, train_accuracy_step=0.945, validation_loss=0.738, validation_accuracy=0.778, train_accuracy_epoch=0.900] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1551: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  50%|█████     | 94/188 [00:42<00:42,  2.19it/s, v_num=2, train_accuracy_step=0.938, validation_loss=0.624, validation_accuracy=0.769, train_accuracy_epoch=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1598: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  75%|███████▌  | 141/188 [01:04<00:21,  2.20it/s, v_num=2, train_accuracy_step=0.898, validation_loss=0.679, validation_accuracy=0.779, train_accuracy_epoch=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1645: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 188/188 [01:25<00:00,  2.21it/s, v_num=2, train_accuracy_step=0.891, validation_loss=0.580, validation_accuracy=0.774, train_accuracy_epoch=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1692: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  25%|██▌       | 47/188 [00:21<01:03,  2.21it/s, v_num=2, train_accuracy_step=0.945, validation_loss=0.667, validation_accuracy=0.761, train_accuracy_epoch=0.913] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1739: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  50%|█████     | 94/188 [00:43<00:43,  2.18it/s, v_num=2, train_accuracy_step=0.961, validation_loss=0.753, validation_accuracy=0.777, train_accuracy_epoch=0.913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1786: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  75%|███████▌  | 141/188 [01:03<00:21,  2.22it/s, v_num=2, train_accuracy_step=0.945, validation_loss=0.688, validation_accuracy=0.773, train_accuracy_epoch=0.913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1833: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 188/188 [01:24<00:00,  2.23it/s, v_num=2, train_accuracy_step=0.938, validation_loss=0.664, validation_accuracy=0.773, train_accuracy_epoch=0.913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1880: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  25%|██▌       | 47/188 [00:20<01:02,  2.27it/s, v_num=2, train_accuracy_step=0.906, validation_loss=0.686, validation_accuracy=0.779, train_accuracy_epoch=0.931]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1927: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  50%|█████     | 94/188 [00:41<00:41,  2.29it/s, v_num=2, train_accuracy_step=0.891, validation_loss=0.742, validation_accuracy=0.770, train_accuracy_epoch=0.931]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1974: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  75%|███████▌  | 141/188 [01:02<00:20,  2.26it/s, v_num=2, train_accuracy_step=0.984, validation_loss=0.834, validation_accuracy=0.778, train_accuracy_epoch=0.931]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2021: 'validation_accuracy' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  88%|████████▊ | 165/188 [01:11<00:09,  2.32it/s, v_num=2, train_accuracy_step=0.938, validation_loss=0.834, validation_accuracy=0.778, train_accuracy_epoch=0.931]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Loading model best.ckpt\")\n",
    "#best_model = CNNModel.load_from_checkpoint(\"models/best.ckpt\", num_classes=1, lr=0.0001).to('cuda')\n",
    "#best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model best.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanis/paris_cite/S2/TER/model_training/venv/lib/python3.12/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_tiny_patch16_224_in21k to current vit_tiny_patch16_224.augreg_in21k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (pretrained_ViT): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=192, out_features=1, bias=True)\n",
       "  )\n",
       "  (accuracy_metric): BinaryAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loading model best.ckpt\")\n",
    "best_model = ViTModel.load_from_checkpoint(\"models/best-v1.ckpt\", num_classes=1, lr=0.0001).to('cuda')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input.to('cuda')\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = label.to('cuda')\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every INPUT_SIZE / Prediction size do the prediction then append\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "        #input = input.unsqueeze(0).to('cpu')\n",
    "        #print(input)\n",
    "        y_hat = best_model(input).squeeze()\n",
    "        result = torch.nn.functional.sigmoid(y_hat)\n",
    "# Output the last 100 of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9351, 0.9714, 0.3627, 0.1418, 0.5261, 0.9647, 0.6980, 0.4075, 0.7207,\n",
       "        0.5179, 0.0435, 0.5058, 0.0073, 0.9472, 0.8690, 0.5513, 0.9751, 0.9591,\n",
       "        0.6555, 0.8751, 0.8892, 0.6918, 0.4900, 0.0498, 0.7538, 0.9683, 0.6867,\n",
       "        0.9061, 0.9249, 0.9157, 0.1855, 0.7719, 0.9694, 0.7420, 0.9255, 0.0397,\n",
       "        0.7750, 0.8253, 0.6235, 0.6668, 0.6615, 0.2057, 0.2617, 0.0080, 0.8687,\n",
       "        0.9533, 0.3209, 0.0161, 0.0356, 0.0656, 0.9721, 0.9669, 0.2779, 0.8716,\n",
       "        0.9646, 0.3607, 0.9113, 0.9637, 0.9039, 0.7049, 0.5602, 0.6361, 0.0273,\n",
       "        0.9400, 0.2535, 0.7930, 0.0390, 0.8970, 0.9526, 0.9416, 0.4072, 0.0072,\n",
       "        0.0093, 0.3741, 0.9503, 0.3566, 0.8670, 0.3487, 0.1036, 0.4163, 0.9888,\n",
       "        0.9069, 0.7401, 0.0678, 0.9539, 0.4978, 0.8160, 0.9348, 0.8016, 0.9649,\n",
       "        0.9311, 0.2102, 0.9488, 0.0719, 0.9170, 0.8647, 0.8692, 0.5067, 0.3115,\n",
       "        0.0758, 0.9592, 0.4297, 0.9075, 0.0162, 0.2135, 0.7954, 0.1990, 0.0786,\n",
       "        0.0626, 0.7620, 0.0085, 0.0726, 0.4273, 0.4229, 0.9776, 0.0235, 0.0712,\n",
       "        0.9103, 0.9200, 0.9606, 0.8916, 0.8375, 0.2391, 0.9080, 0.1865, 0.9112,\n",
       "        0.8205, 0.0308], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
